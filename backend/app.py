# Flask åº”ç”¨ä¸»å…¥å£æ–‡ä»¶
# å®šä¹‰æ‰€æœ‰ API è·¯ç”±

from flask import Flask, request, jsonify, Response, send_from_directory
from flask_cors import CORS
import os
import uuid
import time as time_module  # ä½¿ç”¨åˆ«åé¿å…æ½œåœ¨çš„å‘½åå†²çª
import threading
from concurrent.futures import ThreadPoolExecutor
import base64
import io
import numpy as np
import json
import re
from datetime import datetime
from werkzeug.utils import secure_filename

# PyTorch and related imports
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# PIL for image processing
try:
    from PIL import Image
except ImportError:
    print("âš ï¸ PILåº“æœªå®‰è£…ï¼Œæ‰‹å†™è¯†åˆ«åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    Image = None

# Project-specific imports
from models import get_model_instance
from core.persistence import PersistenceManager

app = Flask(__name__)
CORS(app)

# --- è·¯å¾„é…ç½® (Path Configuration) ---
# ä½¿ç”¨ __file__ è·å– app.py çš„ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿è·¯å¾„çš„å‡†ç¡®æ€§
# os.path.dirname(...) è·å–æ–‡ä»¶æ‰€åœ¨çš„ç›®å½• (å³ backend ç›®å½•)
APP_DIR = os.path.dirname(os.path.abspath(__file__))
# åŸºäº backend ç›®å½•ï¼Œæ„å»ºæ¨¡å‹å’Œæ£€æŸ¥ç‚¹ç›®å½•çš„ç»å¯¹è·¯å¾„
SAVED_MODELS_DIR = os.path.join(APP_DIR, 'saved_models')
CHECKPOINTS_DIR = os.path.join(APP_DIR, 'checkpoints')

# é…ç½®Flaskåº”ç”¨ä»¥æ­£ç¡®å¤„ç†ä¸­æ–‡JSONè¾“å‡º
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True

# å…¨å±€çŠ¶æ€ç®¡ç†
TRAINING_JOBS = {}
LOADED_MODELS = {}
# å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä¸º5
MAX_CONCURRENT_TRAINING_JOBS = int(os.environ.get('MAX_CONCURRENT_TRAINING_JOBS', 5))
TRAINING_EXECUTOR = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_TRAINING_JOBS)
TRAINING_LOCK = threading.Lock()
PERSISTENCE_MANAGER = PersistenceManager()

# 6ä¸ªæ¨¡å‹çš„é…ç½®ä¿¡æ¯
AVAILABLE_MODELS = [
    {
        "id": "mlp",
        "name": "MLP (å¤šå±‚æ„ŸçŸ¥æœº)",
        "description": "æœ€ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ",
        "has_attention": False,
        "parameter_count": 79510
    },
    {
        "id": "cnn",
        "name": "CNN (å·ç§¯ç¥ç»ç½‘ç»œ)",
        "description": "ä¸“ä¸ºå›¾åƒå¤„ç†è®¾è®¡çš„ç»å…¸æ¶æ„",
        "has_attention": False,
        "parameter_count": 94214
    },
    {
        "id": "rnn",
        "name": "RNN (å¾ªç¯ç¥ç»ç½‘ç»œ)",
        "description": "å°†å›¾åƒæŒ‰è¡Œåºåˆ—åŒ–å¤„ç†çš„å®éªŒæ€§æ–¹æ³•",
        "has_attention": False,
        "parameter_count": 127626
    },
    {
        "id": "mlp_attention",
        "name": "MLP + Attention",
        "description": "åœ¨MLPåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶",
        "has_attention": True,
        "parameter_count": 85638
    },
    {
        "id": "cnn_attention",
        "name": "CNN + Attention",
        "description": "åœ¨CNNåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°ç©ºé—´æ³¨æ„åŠ›",
        "has_attention": True,
        "parameter_count": 102350
    },
    {
        "id": "rnn_attention",
        "name": "RNN + Attention",
        "description": "åœ¨RNNåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºåºåˆ—å»ºæ¨¡èƒ½åŠ›",
        "has_attention": True,
        "parameter_count": 134792
    }
]

@app.route('/api/status', methods=['GET'])
def get_status():
    """æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€"""
    return jsonify({
        "status": "running",
        "message": "MNISTæ™ºèƒ½åˆ†æå¹³å°åç«¯æœåŠ¡æ­£å¸¸è¿è¡Œ",
        "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S")
    })

@app.route('/api/models', methods=['GET'])
def get_models():
    """è·å–å¯é€‰æ¨¡å‹åˆ—è¡¨"""
    try:
        # ä½¿ç”¨json.dumpsç¡®ä¿ä¸­æ–‡å­—ç¬¦æ­£ç¡®è¾“å‡º
        json_str = json.dumps(AVAILABLE_MODELS, ensure_ascii=False, indent=2)
        return Response(json_str, mimetype='application/json; charset=utf-8')
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/train', methods=['POST'])
def start_training():
    """å¯åŠ¨æ¨¡å‹è®­ç»ƒ"""
    try:
        data = request.get_json()
        if not data or 'models' not in data:
            return jsonify({"error": "è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ï¼Œéœ€è¦åŒ…å«modelså­—æ®µ"}), 400
        
        training_configs = data['models']
        jobs = []
        
        for config in training_configs:
            # ç”Ÿæˆå”¯ä¸€çš„job_id
            job_id = f"job_{config['id']}_{int(time_module.time() * 1000)}"
            
            # åˆ›å»ºä»»åŠ¡ä¿¡æ¯
            job_info = {
                "job_id": job_id,
                "model_id": config['id'],
                "status": "queued",
                "start_time": time_module.time(),
                "end_time": None,
                "progress": {
                    "percentage": 0,
                    "current_epoch": 0,
                    "total_epochs": config.get('epochs', 10),
                    "accuracy": 0.0,
                    "loss": 0.0,
                    "best_accuracy": 0.0,
                    "historical_best_accuracy": 0.0,
                    "is_first_training": True
                },
                "config": {
                    "epochs": config.get('epochs', 10),
                    "learning_rate": config.get('lr', 0.001),
                    "batch_size": config.get('batch_size', 64)
                },
                "error_message": None
            }
            
            # ä¿å­˜åˆ°å…¨å±€çŠ¶æ€
            TRAINING_JOBS[job_id] = job_info
            
            jobs.append({
                "job_id": job_id,
                "model_id": config['id']
            })
            
            # å¯åŠ¨å®é™…çš„è®­ç»ƒä»»åŠ¡
            TRAINING_EXECUTOR.submit(safe_training_wrapper, job_id, config['id'], config.get('epochs', 10), config.get('lr', 0.001), config.get('batch_size', 64))
        
        return jsonify({"jobs": jobs})
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def safe_training_wrapper(job_id, model_id, epochs, lr, batch_size):
    """å®‰å…¨çš„è®­ç»ƒåŒ…è£…å‡½æ•°ï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†"""
    try:
        print(f"ğŸš€ å¯åŠ¨è®­ç»ƒä»»åŠ¡ {job_id}ï¼šæ¨¡å‹ {model_id}, Epochs: {epochs}, LR: {lr}, Batch Size: {batch_size}")
        
        # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'running'
                TRAINING_JOBS[job_id]['start_time'] = time_module.time()
        
        # æ‰§è¡ŒçœŸå®è®­ç»ƒ
        perform_real_training(job_id, model_id, epochs, lr, batch_size)
        
        print(f"âœ… è®­ç»ƒä»»åŠ¡ {job_id} æˆåŠŸå®Œæˆ")
        
    except Exception as e:
        error_message = f"è®­ç»ƒå¤±è´¥: {str(e)}"
        print(f"âŒ è®­ç»ƒä»»åŠ¡ {job_id} å¤±è´¥: {error_message}")
        
        # æ›´æ–°é”™è¯¯çŠ¶æ€
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'error'
                TRAINING_JOBS[job_id]['error_message'] = error_message
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()

def run_training_job(job_id, model_id, epochs, lr, batch_size):
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œçš„è®­ç»ƒä»»åŠ¡ - å·²å¼ƒç”¨ï¼Œä½¿ç”¨ safe_training_wrapper"""
    pass

def is_first_training_for_model(model_id, save_dir):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæŸæ¨¡å‹çš„é¦–æ¬¡è®­ç»ƒ
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆå¦‚'cnn', 'mlp'ç­‰ï¼‰
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•è·¯å¾„
    
    Returns:
        tuple: (is_first_training: bool, historical_best_accuracy: float)
    """
    try:
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)
            return True, 0.0
        
        # æ‰«æå·²æœ‰æ¨¡å‹æ–‡ä»¶
        model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth')]
        
        # æŸ¥æ‰¾ç›¸åŒæ¨¡å‹ç±»å‹çš„æ–‡ä»¶
        matching_files = []
        for filename in model_files:
            if filename.startswith(f"{model_id}_"):
                matching_files.append(filename)
        
        if not matching_files:
            return True, 0.0
        
        # æå–å†å²æœ€ä½³å‡†ç¡®ç‡
        best_accuracy = 0.0
        for filename in matching_files:
            accuracy = extract_accuracy_from_filename(filename)
            if accuracy > best_accuracy:
                best_accuracy = accuracy
        
        return False, best_accuracy
        
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥é¦–æ¬¡è®­ç»ƒçŠ¶æ€å¤±è´¥: {e}")
        return True, 0.0

def extract_accuracy_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
    
    Args:
        filename: æ¨¡å‹æ–‡ä»¶å
    
    Returns:
        float: å‡†ç¡®ç‡ï¼Œè§£æå¤±è´¥æ—¶è¿”å›0.0
    """
    try:
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1. model_id_best_acc_0.9947.pth
        # 2. model_id_20250618_232011_acc_0.9947.pth
        
        if '_acc_' in filename:
            # æ‰¾åˆ°æœ€åä¸€ä¸ª '_acc_'
            last_acc_index = filename.rfind('_acc_')
            accuracy_part = filename[last_acc_index + 5:].replace('.pth', '')
            return float(accuracy_part)
        
        return 0.0
        
    except (ValueError, IndexError):
        return 0.0

def save_model_with_replacement(model, model_id, accuracy, save_dir, job_id):
    """ä¿å­˜æ¨¡å‹ï¼Œå¦‚æœæ€§èƒ½æ›´å¥½åˆ™æ›¿æ¢æ—§æ¨¡å‹
    
    Args:
        model: è¦ä¿å­˜çš„PyTorchæ¨¡å‹
        model_id: æ¨¡å‹ID
        accuracy: å½“å‰æ¨¡å‹çš„å‡†ç¡®ç‡
        save_dir: ä¿å­˜ç›®å½•
        job_id: è®­ç»ƒä»»åŠ¡ID
    """
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è®­ç»ƒ
        is_first, historical_best = is_first_training_for_model(model_id, save_dir)
        
        # æ„å»ºæ–°çš„æ–‡ä»¶åï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
        new_filename = f"{model_id}_best_acc_{accuracy:.4f}.pth"
        new_filepath = os.path.join(save_dir, new_filename)
        
        # å¦‚æœä¸æ˜¯é¦–æ¬¡è®­ç»ƒï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦è¶…è¶Šå†å²æœ€ä½³
        if not is_first:
            if accuracy <= historical_best:
                print(f"âš ï¸ å½“å‰å‡†ç¡®ç‡ {accuracy:.4f} æœªè¶…è¶Šå†å²æœ€ä½³ {historical_best:.4f}")
                return False
            
            # åˆ é™¤æ—§çš„æ¨¡å‹æ–‡ä»¶
            model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth') and f.startswith(f"{model_id}_")]
            for old_file in model_files:
                old_filepath = os.path.join(save_dir, old_file)
                if os.path.exists(old_filepath):
                    os.remove(old_filepath)
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ¨¡å‹æ–‡ä»¶: {old_file}")
        
        # ä¿å­˜æ–°æ¨¡å‹
        torch.save(model.state_dict(), new_filepath)
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜æˆåŠŸ: {new_filename}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(new_filepath) / 1024 / 1024:.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        return False

def find_optimal_batch_size(model, device, base_batch_size=64):
    """åŠ¨æ€å¯»æ‰¾æœ€ä¼˜æ‰¹é‡å¤§å°
    
    Args:
        model: PyTorchæ¨¡å‹
        device: è®¾å¤‡ (cuda/cpu)
        base_batch_size: åŸºç¡€æ‰¹é‡å¤§å°
    
    Returns:
        int: æœ€ä¼˜æ‰¹é‡å¤§å°
    """
    if device.type == 'cpu':
        return min(base_batch_size, 32)  # CPUé™åˆ¶æ›´ä¸¥æ ¼
    
    optimal_batch_size = base_batch_size
    
    try:
        # å°è¯•é€æ­¥å¢åŠ æ‰¹é‡å¤§å°ç›´åˆ°å†…å­˜ä¸è¶³
        for test_batch_size in [64, 128, 256, 512]:
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_input = torch.randn(test_batch_size, 1, 28, 28).to(device)
                
                # å‰å‘ä¼ æ’­æµ‹è¯•
                model.eval()
                with torch.no_grad():
                    _ = model(test_input)
                
                optimal_batch_size = test_batch_size
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    break
                else:
                    raise e
    except Exception as e:
        print(f"âš ï¸ æ‰¹é‡å¤§å°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
    
    return optimal_batch_size

def perform_real_training(job_id, model_id, epochs, lr, batch_size):
    """æ‰§è¡ŒçœŸå®çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”© ä½¿ç”¨è®¾å¤‡: {device}")

    # --- è·¯å¾„è®¾ç½® ---
    # ä½¿ç”¨åœ¨æ–‡ä»¶é¡¶éƒ¨å®šä¹‰çš„å…¨å±€è·¯å¾„å¸¸é‡
    os.makedirs(SAVED_MODELS_DIR, exist_ok=True)
    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)
    
    # è·å–å†å²æœ€ä½³å‡†ç¡®ç‡
    is_first_train, historical_best_accuracy = is_first_training_for_model(model_id, SAVED_MODELS_DIR)
    
    with TRAINING_LOCK:
        TRAINING_JOBS[job_id]['progress']['historical_best_accuracy'] = historical_best_accuracy
        TRAINING_JOBS[job_id]['progress']['is_first_training'] = is_first_train

    print(f"ğŸ“š æ¨¡å‹ {model_id} çš„å†å²æœ€ä½³å‡†ç¡®ç‡: {historical_best_accuracy:.4f}")

    train_start_time = time_module.time()
    model = None
    final_accuracy = 0.0
    is_new_record = False
    
    try:
        print(f"ğŸ¯ å¼€å§‹çœŸå®è®­ç»ƒ: {model_id}")
        
        # 1. è®¾å¤‡æ£€æµ‹ã€æ•°æ®åŠ è½½ç­‰...
        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST('data', train=False, transform=transform)
        
        model_instance = get_model_instance(model_id)
        model_instance.to(device)
        optimal_batch_size = find_optimal_batch_size(model_instance, device, batch_size)
        
        train_loader = DataLoader(train_dataset, batch_size=optimal_batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=optimal_batch_size, shuffle=False)
        
        model = get_model_instance(model_id).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr)
        
        best_accuracy = 0.0
        
        # 2. è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            model.train()
            batch_start_time = time_module.time()
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(device), target.to(device)
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                # æ¯50ä¸ªæ‰¹æ¬¡æ›´æ–°ä¸€æ¬¡è¿›åº¦
                if batch_idx > 0 and batch_idx % 50 == 0:
                    samples_processed = (batch_idx + 1) * optimal_batch_size
                    time_elapsed = time_module.time() - batch_start_time
                    samples_per_sec = samples_processed / time_elapsed
                    
                    with TRAINING_LOCK:
                        if job_id in TRAINING_JOBS:
                            TRAINING_JOBS[job_id]['progress'].update({
                                'percentage': (epoch * len(train_loader) + batch_idx) / (epochs * len(train_loader)) * 100,
                                'current_epoch': epoch + 1,
                                'loss': loss.item(),
                                'samples_per_sec': samples_per_sec,
                            })
                            TRAINING_JOBS[job_id]['status'] = 'running' # ç¡®ä¿çŠ¶æ€ä¸º running
            
            # ... (æµ‹è¯•é˜¶æ®µä»£ç ä¿æŒä¸å˜) ...
            model.eval()
            test_loss = 0.0
            correct = 0
            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    test_loss += criterion(output, target).item()
                    pred = output.argmax(dim=1, keepdim=True)
                    correct += pred.eq(target.view_as(pred)).sum().item()
            
            test_accuracy = correct / len(test_loader.dataset)
            if test_accuracy > best_accuracy:
                best_accuracy = test_accuracy
                is_new_record = PERSISTENCE_MANAGER.save_checkpoint(model, model_id, job_id, epoch, best_accuracy)

            with TRAINING_LOCK:
                 if job_id in TRAINING_JOBS:
                    TRAINING_JOBS[job_id]['progress'].update({
                        'percentage': ((epoch + 1) / epochs) * 100,
                        'current_epoch': epoch + 1,
                        'accuracy': test_accuracy,
                        'best_accuracy': best_accuracy,
                    })

        final_accuracy = best_accuracy
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'completed'
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()
        
    except Exception as e:
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'error'
                TRAINING_JOBS[job_id]['error_message'] = str(e)
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()
    
    finally:
        # 3. æ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½è®°å½•åˆ°å†å²æ¡£æ¡ˆ
        training_duration = time_module.time() - train_start_time
        history_entry = {
            "job_id": job_id,
            "model_id": model_id,
            "model_name": get_model_display_info(model_id).get('name', model_id),
            "has_attention": get_model_display_info(model_id).get('has_attention', False),
            "status": TRAINING_JOBS.get(job_id, {}).get('status', 'unknown'),
            "start_time": train_start_time,
            "completion_time": time_module.time(),
            "hyperparameters": {
                "epochs": epochs,
                "learning_rate": lr,
                "batch_size": batch_size
            },
            "metrics": {
                "final_accuracy": final_accuracy,
                "training_duration_sec": training_duration,
                "is_new_record": is_new_record,
                "total_params": sum(p.numel() for p in model.parameters()) if model else 0
            },
            "error_message": TRAINING_JOBS.get(job_id, {}).get('error_message')
        }
        PERSISTENCE_MANAGER.append_training_history(history_entry)
        print(f"Hï¸âƒ£ è®­ç»ƒå†å²å·²å­˜æ¡£: {job_id}")

        # åœ¨æ‰€æœ‰epochç»“æŸå
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if best_accuracy > historical_best_accuracy:
            print(f"ğŸ‰ æ–°çºªå½•! å‡†ç¡®ç‡ä» {historical_best_accuracy:.4f} æå‡åˆ° {best_accuracy:.4f}ã€‚æ­£åœ¨ä¿å­˜æ¨¡å‹...")
            save_model_with_replacement(model.state_dict(), model_id, best_accuracy, SAVED_MODELS_DIR, job_id)
        else:
            print(f"ğŸ‘ è®­ç»ƒå®Œæˆï¼Œä½†æœªè¶…è¶Šå†å²æœ€ä½³å‡†ç¡®ç‡({historical_best_accuracy:.4f})")

@app.route('/api/training_history', methods=['GET'])
def get_training_history():
    """æä¾›æ‰€æœ‰è®­ç»ƒå†å²è®°å½•çš„æ¥å£"""
    try:
        history = PERSISTENCE_MANAGER.get_training_history()
        
        # æ•°æ®æ¸…æ´—å’Œå¢å¼ºï¼šç¡®ä¿æ¯æ¡è®°å½•éƒ½æœ‰å¯ç”¨çš„æ—¶é•¿å’ŒISOæ ¼å¼æ—¶é—´
        for record in history:
            # å…¼å®¹å¤„ç† training_duration_sec
            metrics = record.get('metrics', {})
            if 'training_duration_sec' not in metrics:
                start = record.get('start_time')
                end = record.get('completion_time')
                if start and end:
                    metrics['training_duration_sec'] = end - start
                else:
                    metrics['training_duration_sec'] = 0
            record['metrics'] = metrics # ç¡®ä¿ metrics å­—å…¸è¢«å†™å›

            # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ›´æ˜“è¯»çš„ISO 8601æ ¼å¼å­—ç¬¦ä¸²
            if record.get('start_time'):
                record['start_time_iso'] = datetime.fromtimestamp(record['start_time']).isoformat()
            else:
                record['start_time_iso'] = 'N/A' # æä¾›é»˜è®¤å€¼

            if record.get('completion_time'):
                 record['completion_time_iso'] = datetime.fromtimestamp(record['completion_time']).isoformat()
            else:
                record['completion_time_iso'] = 'N/A' # æä¾›é»˜è®¤å€¼

        return jsonify(history)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/training_progress', methods=['GET'])
def get_training_progress():
    """è·å–è®­ç»ƒè¿›åº¦"""
    try:
        job_ids = request.args.get('job_ids')
        if not job_ids:
            return jsonify({"error": "ç¼ºå°‘job_idså‚æ•°"}), 400
        
        job_list = [job_id.strip() for job_id in job_ids.split(',')]
        
        progress_list = []
        
        with TRAINING_LOCK:
            for job_id in job_list:
                if job_id in TRAINING_JOBS:
                    job_info = TRAINING_JOBS[job_id].copy()
                    progress_list.append(job_info)
                else:
                    return jsonify({"error": f"è®­ç»ƒä»»åŠ¡ {job_id} ä¸å­˜åœ¨"}), 404
        
        # è¿”å›å‰ç«¯æœŸæœ›çš„æ ¼å¼
        return jsonify({"progress": progress_list})
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/trained_models', methods=['GET'])
def get_trained_models():
    """è·å–å·²è®­ç»ƒæ¨¡å‹åˆ—è¡¨"""
    try:
        trained_models = scan_trained_models()
        return jsonify(trained_models)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def scan_trained_models():
    """æ‰«æsaved_modelsç›®å½•ä¸­çš„å·²è®­ç»ƒæ¨¡å‹
    
    Returns:
        list: å·²è®­ç»ƒæ¨¡å‹åˆ—è¡¨ï¼Œæ¯ä¸ªæ¨¡å‹åŒ…å«idã€nameã€model_typeã€accuracyç­‰ä¿¡æ¯
    """
    trained_models = []
    save_dir = SAVED_MODELS_DIR # ä½¿ç”¨å…¨å±€è·¯å¾„å¸¸é‡
    
    try:
        if not os.path.exists(save_dir):
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜ç›®å½•ä¸å­˜åœ¨: {save_dir}")
            os.makedirs(save_dir) # å¦‚æœä¸å­˜åœ¨ï¼Œå°±åˆ›å»ºä¸€ä¸ª
            return trained_models
        
        # è·å–ç›®å½•ä¸­çš„æ‰€æœ‰ .pth æ–‡ä»¶
        model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth')]
        
        if not model_files:
            print(f"ğŸ“‚ æ¨¡å‹ä¿å­˜ç›®å½•ä¸ºç©º: {save_dir}")
            return trained_models
        
        print(f"ğŸ” æ‰«æåˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")
        
        for filename in model_files:
            try:
                # æ–¹æ¡ˆ2ï¼šæ‰“é€ ä¸€ä¸ªèƒ½åŒæ—¶å¤„ç†æ–°æ—§ä¸¤ç§å‘½åæ ¼å¼çš„æ™ºèƒ½è§£æå™¨
                # æ­£åˆ™è¡¨è¾¾å¼èƒ½åƒç²¾ç¡®åˆ¶å¯¼ä¸€æ ·ï¼Œä»å¤æ‚æ–‡ä»¶åä¸­æå–æ‰€éœ€ä¿¡æ¯
                # æ¨¡å¼è§£é‡Š:
                #   - r'...' : Pythonä¸­çš„åŸç”Ÿå­—ç¬¦ä¸²ï¼Œé¿å…åæ–œæ é—®é¢˜
                #   - _(?:best_)?acc_ : è¿™æ˜¯ä¸€ä¸ªéæ•è·ç»„ (?:...)ï¼ŒåŒ¹é… "_acc_" æˆ– "_best_acc_"
                #                      "best_" éƒ¨åˆ†æ˜¯å¯é€‰çš„ (?)
                delimiter_pattern = r'_(?:best_)?acc_'
                parts = re.split(delimiter_pattern, filename)

                model_id = ""
                accuracy = 0.0

                if len(parts) == 2:
                    # åŒ¹é…æˆåŠŸï¼Œä¾‹å¦‚ "cnn_attention_best_acc_0.9947.pth"
                    # parts ä¼šæ˜¯ ['cnn_attention', '0.9947.pth']
                    model_id = parts[0]
                    accuracy = float(parts[1].replace('.pth', ''))
                elif len(parts) == 1:
                    # æ— æ³•ç”¨ç²¾åº¦åˆ†å‰²ï¼Œè¯´æ˜å¯èƒ½æ˜¯ "mlp.pth" è¿™æ ·çš„ç®€å•æ ¼å¼
                    model_id = filename.replace('.pth', '')
                    accuracy = 0.0  # ç»™äºˆä¸€ä¸ªé»˜è®¤å€¼ï¼Œè¡¨ç¤ºç²¾åº¦æœªçŸ¥
                else:
                    # æ ¼å¼æ— æ³•è¯†åˆ«ï¼Œè·³è¿‡
                    print(f"âš ï¸ æ–‡ä»¶åæ ¼å¼æ— æ³•è¯†åˆ«ï¼Œå·²è·³è¿‡: {filename}")
                    continue

                # è·å–æ¨¡å‹æ˜¾ç¤ºåç§°å’Œä¿¡æ¯
                model_info = get_model_display_info(model_id)
                if not model_info:
                    print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ '{model_id}' çš„é…ç½®ä¿¡æ¯ï¼Œå·²è·³è¿‡: {filename}")
                    continue
                
                # è·å–æ–‡ä»¶ä¿¡æ¯
                file_path = os.path.join(save_dir, filename)
                file_stats = os.stat(file_path)
                file_size = file_stats.st_size
                
                # æ ¼å¼åŒ–è®­ç»ƒæ—¶é—´ï¼ˆä½¿ç”¨time_moduleé¿å…å‘½åå†²çªï¼‰
                mtime = file_stats.st_mtime
                local_time = time_module.localtime(mtime)
                training_time = time_module.strftime('%Y-%m-%dT%H:%M:%S', local_time)
                
                trained_model = {
                    "id": filename.replace('.pth', ''),  # ä½¿ç”¨å®Œæ•´æ–‡ä»¶åä½œä¸ºID
                    "name": f"{model_info['name']} (å‡†ç¡®ç‡: {accuracy:.2%})",
                    "model_type": model_id,
                    "has_attention": model_info['has_attention'],
                    "accuracy": accuracy,
                    "training_time": training_time,
                    "file_size": file_size,
                    "parameter_count": model_info['parameter_count']
                }
                
                trained_models.append(trained_model)
                print(f"âœ… å‘ç°æ¨¡å‹: {model_id} - å‡†ç¡®ç‡: {accuracy:.4f}")
                
            except Exception as e:
                print(f"âš ï¸ è§£ææ¨¡å‹æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                continue
        
        # æŒ‰å‡†ç¡®ç‡é™åºæ’åº
        trained_models.sort(key=lambda x: x['accuracy'], reverse=True)
        
        print(f"ğŸ“‹ æˆåŠŸæ‰«æ {len(trained_models)} ä¸ªæœ‰æ•ˆçš„å·²è®­ç»ƒæ¨¡å‹")
        return trained_models
        
    except Exception as e:
        print(f"âŒ æ‰«æå·²è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
        return []

def get_model_display_info(model_id):
    """è·å–æ¨¡å‹çš„æ˜¾ç¤ºä¿¡æ¯
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆå¦‚'cnn', 'mlp'ç­‰ï¼‰
    
    Returns:
        dict: æ¨¡å‹æ˜¾ç¤ºä¿¡æ¯ï¼Œå¦‚æœæ¨¡å‹IDæ— æ•ˆåˆ™è¿”å›None
    """
    # åœ¨å…¨å±€æ¨¡å‹é…ç½®ä¸­æŸ¥æ‰¾
    for model_config in AVAILABLE_MODELS:
        if model_config['id'] == model_id:
            return {
                'name': model_config['name'],
                'has_attention': model_config['has_attention'],
                'parameter_count': model_config['parameter_count']
            }
    
    print(f"âš ï¸ æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_id}")
    return None

@app.route('/api/predict', methods=['POST'])
def predict():
    """æ‰§è¡Œæ‰‹å†™è¯†åˆ«"""
    try:
        data = request.get_json()
        if not data or 'model_id' not in data or 'image_base64' not in data:
            return jsonify({"error": "è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ï¼Œéœ€è¦åŒ…å«model_idå’Œimage_base64å­—æ®µ"}), 400
        
        model_id = data['model_id']
        image_base64 = data['image_base64']
        
        print(f"ğŸ” å¼€å§‹é¢„æµ‹ï¼Œæ¨¡å‹: {model_id}")
        
        # åŠ è½½æ¨¡å‹
        model = load_model_for_prediction(model_id)
        if model is None:
            return jsonify({"error": f"æ— æ³•åŠ è½½æ¨¡å‹: {model_id}"}), 404
        
        # é¢„å¤„ç†å›¾åƒ
        input_tensor = preprocess_canvas_image(image_base64)
        if input_tensor is None:
            return jsonify({"error": "å›¾åƒé¢„å¤„ç†å¤±è´¥"}), 400
        
        # æ‰§è¡Œæ¨ç†
        prediction, probabilities = perform_inference(model, input_tensor)
        
        result = {
            "prediction": int(prediction),
            "probabilities": probabilities.tolist(),
            "confidence": float(probabilities.max())
        }
        
        print(f"âœ… é¢„æµ‹å®Œæˆ: {prediction} (ç½®ä¿¡åº¦: {probabilities.max():.4f})")
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        return jsonify({"error": f"é¢„æµ‹å¤±è´¥: {str(e)}"}), 500

def load_model_for_prediction(model_id):
    """åŠ è½½æŒ‡å®šçš„æ¨¡å‹ç”¨äºé¢„æµ‹
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆå®Œæ•´æ–‡ä»¶åï¼Œä¸å«.pthæ‰©å±•åï¼‰
    
    Returns:
        torch.nn.Module: åŠ è½½çš„æ¨¡å‹ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if model_id in LOADED_MODELS:
            print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹: {model_id}")
            return LOADED_MODELS[model_id]
        
        # ä½¿ç”¨å…¨å±€è·¯å¾„å¸¸é‡æ„å»ºæ¨¡å‹æ–‡ä»¶è·¯å¾„
        model_path = os.path.join(SAVED_MODELS_DIR, f"{model_id}.pth")
        
        if not os.path.exists(model_path):
            print(f"âŒ é¢„æµ‹æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
            
        print(f"ê°™ì€ ëª¨ë¸ ë¡œë”©: {model_path}")

        # --- æ™ºèƒ½è§£ææ¨¡å‹ç±»å‹ (Smartly Parse Model Type) ---
        # ä»å®Œæ•´æ–‡ä»¶åä¸­æå–åŸºç¡€æ¨¡å‹IDï¼Œä¾‹å¦‚ä» 'cnn_attention_best_acc_0.9947' æå– 'cnn_attention'
        delimiter_pattern = r'_(?:best_)?acc_'
        parts = re.split(delimiter_pattern, model_id)
        model_type = parts[0]

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: '{model_type}' (ä» '{model_id}' è§£æ)")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model_instance = get_model_instance(model_type)
        
        # åŠ è½½æƒé‡
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            model_instance.load_state_dict(checkpoint)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            model_instance.eval()
            
            # ç¼“å­˜æ¨¡å‹ï¼ˆé™åˆ¶ç¼“å­˜æ•°é‡é¿å…å†…å­˜è¿‡å¤šï¼‰
            if len(LOADED_MODELS) >= 5:  # æœ€å¤šç¼“å­˜5ä¸ªæ¨¡å‹
                # åˆ é™¤æœ€æ—§çš„æ¨¡å‹
                oldest_key = next(iter(LOADED_MODELS))
                del LOADED_MODELS[oldest_key]
                print(f"ğŸ—‘ï¸ åˆ é™¤ç¼“å­˜æ¨¡å‹: {oldest_key}")
            
            LOADED_MODELS[model_id] = model_instance
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸå¹¶ç¼“å­˜: {model_id}")
            
            return model_instance
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
            return None
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_id}: {e}")
        return None

def preprocess_canvas_image(image_base64):
    """é¢„å¤„ç†Canvaså›¾åƒæ•°æ®
    
    Args:
        image_base64: base64ç¼–ç çš„å›¾åƒæ•°æ®
    
    Returns:
        torch.Tensor: é¢„å¤„ç†åçš„tensorï¼Œå½¢çŠ¶ä¸º(1, 1, 28, 28)ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # è§£ç base64å›¾åƒ
        if image_base64.startswith('data:image/'):
            # ç§»é™¤data URLå‰ç¼€
            image_base64 = image_base64.split(',')[1]
        
        # base64è§£ç 
        image_data = base64.b64decode(image_base64)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        image = Image.open(io.BytesIO(image_data))
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        if image.mode != 'L':
            image = image.convert('L')
        
        # è°ƒæ•´å¤§å°åˆ°28x28
        image = image.resize((28, 28), Image.LANCZOS)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        image_array = np.array(image)
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        image_array = image_array.astype(np.float32) / 255.0
        
        # åº”ç”¨MNISTæ ‡å‡†å½’ä¸€åŒ–ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        image_array = (image_array - 0.1307) / 0.3081
        
        # è½¬æ¢ä¸ºPyTorch tensor
        image_tensor = torch.from_numpy(image_array)
        
        # æ·»åŠ batchå’Œchannelç»´åº¦: (28, 28) -> (1, 1, 28, 28)
        image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)
        
        print(f"ğŸ“· å›¾åƒé¢„å¤„ç†å®Œæˆï¼Œtensorå½¢çŠ¶: {image_tensor.shape}")
        return image_tensor
        
    except Exception as e:
        print(f"âŒ å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
        return None

def perform_inference(model, input_tensor):
    """æ‰§è¡Œæ¨¡å‹æ¨ç†
    
    Args:
        model: PyTorchæ¨¡å‹
        input_tensor: è¾“å…¥tensor
    
    Returns:
        tuple: (prediction, probabilities) - é¢„æµ‹ç»“æœå’Œæ¦‚ç‡åˆ†å¸ƒ
    """
    try:
        with torch.no_grad():
            # å‰å‘ä¼ æ’­
            outputs = model(input_tensor)
            
            # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
            probabilities = torch.softmax(outputs, dim=1)
            
            # è·å–é¢„æµ‹ç»“æœ
            prediction = torch.argmax(probabilities, dim=1)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            probabilities = probabilities.squeeze().numpy()
            prediction = prediction.item()
            
            return prediction, probabilities
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        raise e

def check_system_health():
    """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        health_info = {
            "status": "healthy",
            "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S"),
            "active_training_jobs": len([job for job in TRAINING_JOBS.values() if job['status'] == 'running']),
            "loaded_models": len(LOADED_MODELS),
            "available_models": len(AVAILABLE_MODELS)
        }
        
        return health_info
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S")
        }

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨MNISTæ™ºèƒ½åˆ†æå¹³å°åç«¯æœåŠ¡...")
    print("ğŸ”§ ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
    
    health = check_system_health()
    print(f"âœ… ç³»ç»ŸçŠ¶æ€: {health['status']}")
    print(f"ğŸ“Š å¯ç”¨æ¨¡å‹: {health['available_models']} ä¸ª")
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(host='0.0.0.0', port=5000, debug=True) 