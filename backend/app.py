# Flask åº”ç”¨ä¸»å…¥å£æ–‡ä»¶
# å®šä¹‰æ‰€æœ‰ API è·¯ç”±

from flask import Flask, request, jsonify, Response, send_from_directory
from flask_cors import CORS
import os
import uuid
import time as time_module  # ä½¿ç”¨åˆ«åé¿å…æ½œåœ¨çš„å‘½åå†²çª
import threading
from concurrent.futures import ThreadPoolExecutor
import base64
import io
import numpy as np
import json
import re
from datetime import datetime
from werkzeug.utils import secure_filename

# PyTorch and related imports
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# PIL for image processing
try:
    from PIL import Image
except ImportError:
    print("âš ï¸ PILåº“æœªå®‰è£…ï¼Œæ‰‹å†™è¯†åˆ«åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    Image = None

# Project-specific imports
from models import get_model_instance
from core.persistence import PersistenceManager

app = Flask(__name__)
CORS(app)

# --- è·¯å¾„é…ç½® (Path Configuration) ---
# ä½¿ç”¨ __file__ è·å– app.py çš„ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿è·¯å¾„çš„å‡†ç¡®æ€§
# os.path.dirname(...) è·å–æ–‡ä»¶æ‰€åœ¨çš„ç›®å½• (å³ backend ç›®å½•)
APP_DIR = os.path.dirname(os.path.abspath(__file__))
# åŸºäº backend ç›®å½•ï¼Œæ„å»ºæ¨¡å‹å’Œæ£€æŸ¥ç‚¹ç›®å½•çš„ç»å¯¹è·¯å¾„
SAVED_MODELS_DIR = os.path.join(APP_DIR, 'saved_models')
CHECKPOINTS_DIR = os.path.join(APP_DIR, 'checkpoints')

# é…ç½®Flaskåº”ç”¨ä»¥æ­£ç¡®å¤„ç†ä¸­æ–‡JSONè¾“å‡º
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True

# å…¨å±€çŠ¶æ€ç®¡ç†
TRAINING_JOBS = {}
LOADED_MODELS = {}
# å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä¸º5
MAX_CONCURRENT_TRAINING_JOBS = int(os.environ.get('MAX_CONCURRENT_TRAINING_JOBS', 5))
TRAINING_EXECUTOR = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_TRAINING_JOBS)
TRAINING_LOCK = threading.Lock()
# å°†æŒä¹…åŒ–ç›®å½•å›ºå®šåˆ° backend ç›®å½•ï¼Œå¹¶å°†é”æ³¨å…¥ç®¡ç†å™¨
PERSISTENCE_MANAGER = PersistenceManager(APP_DIR, lock=TRAINING_LOCK)

# 6ä¸ªæ¨¡å‹çš„é…ç½®ä¿¡æ¯
AVAILABLE_MODELS = [
    {
        "id": "mlp",
        "name": "MLP (å¤šå±‚æ„ŸçŸ¥æœº)",
        "description": "æœ€ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ",
        "has_attention": False,
        "parameter_count": 79510
    },
    {
        "id": "cnn",
        "name": "CNN (å·ç§¯ç¥ç»ç½‘ç»œ)",
        "description": "ä¸“ä¸ºå›¾åƒå¤„ç†è®¾è®¡çš„ç»å…¸æ¶æ„",
        "has_attention": False,
        "parameter_count": 94214
    },
    {
        "id": "rnn",
        "name": "RNN (å¾ªç¯ç¥ç»ç½‘ç»œ)",
        "description": "å°†å›¾åƒæŒ‰è¡Œåºåˆ—åŒ–å¤„ç†çš„å®éªŒæ€§æ–¹æ³•",
        "has_attention": False,
        "parameter_count": 127626
    },
    {
        "id": "mlp_attention",
        "name": "MLP + Attention",
        "description": "åœ¨MLPåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶",
        "has_attention": True,
        "parameter_count": 85638
    },
    {
        "id": "cnn_attention",
        "name": "CNN + Attention",
        "description": "åœ¨CNNåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°ç©ºé—´æ³¨æ„åŠ›",
        "has_attention": True,
        "parameter_count": 102350
    },
    {
        "id": "rnn_attention",
        "name": "RNN + Attention",
        "description": "åœ¨RNNåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºåºåˆ—å»ºæ¨¡èƒ½åŠ›",
        "has_attention": True,
        "parameter_count": 134792
    }
]

# æ–°å¢å·¥å…·å‡½æ•°ï¼Œæ–¹ä¾¿é€šè¿‡IDæŸ¥æ‰¾åç§°
def get_model_name_by_id(model_id):
    """æ ¹æ®æ¨¡å‹IDè·å–æ¨¡å‹æ˜¾ç¤ºåç§°"""
    for model in AVAILABLE_MODELS:
        if model['id'] == model_id:
            return model['name']
    return model_id # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›åŸå§‹ID

@app.route('/api/status', methods=['GET'])
def get_status():
    """æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€"""
    return jsonify({
        "status": "running",
        "message": "MNISTæ™ºèƒ½åˆ†æå¹³å°åç«¯æœåŠ¡æ­£å¸¸è¿è¡Œ",
        "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S")
    })

@app.route('/api/models', methods=['GET'])
def get_models():
    """è·å–å¯é€‰æ¨¡å‹åˆ—è¡¨"""
    try:
        # ä½¿ç”¨json.dumpsç¡®ä¿ä¸­æ–‡å­—ç¬¦æ­£ç¡®è¾“å‡º
        json_str = json.dumps(AVAILABLE_MODELS, ensure_ascii=False, indent=2)
        return Response(json_str, mimetype='application/json; charset=utf-8')
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/train', methods=['POST'])
def start_training():
    """å¯åŠ¨æ¨¡å‹è®­ç»ƒ"""
    try:
        data = request.get_json()
        if not data or 'models' not in data:
            return jsonify({"error": "è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ï¼Œéœ€è¦åŒ…å«modelså­—æ®µ"}), 400
        
        training_configs = data['models']
        jobs = []
        
        for config in training_configs:
            # ç”Ÿæˆå”¯ä¸€çš„job_id
            job_id = f"job_{config['id']}_{int(time_module.time() * 1000)}"
            
            # åˆ›å»ºä»»åŠ¡ä¿¡æ¯
            job_info = {
                "job_id": job_id,
                "model_id": config['id'],
                "status": "queued",
                "start_time": time_module.time(),
                "end_time": None,
                "progress": {
                    "percentage": 0,
                    "current_epoch": 0,
                    "total_epochs": config.get('epochs', 10),
                    "accuracy": 0.0,
                    "loss": 0.0,
                    "best_accuracy": 0.0,
                    "samples_per_second": 0,
                    "historical_best_accuracy": 0.0,
                    "is_first_training": True
                },
                "config": {
                    "epochs": config.get('epochs', 10),
                    "learning_rate": config.get('lr', 0.001),
                    "batch_size": config.get('batch_size', 64)
                },
                "error_message": None
            }
            
            # ä¿å­˜åˆ°å…¨å±€çŠ¶æ€
            TRAINING_JOBS[job_id] = job_info
            
            jobs.append({
                "job_id": job_id,
                "model_id": config['id']
            })
            
            # å¯åŠ¨å®é™…çš„è®­ç»ƒä»»åŠ¡
            TRAINING_EXECUTOR.submit(safe_training_wrapper, job_id, config['id'], config.get('epochs', 10), config.get('lr', 0.001), config.get('batch_size', 64))
        
        return jsonify({"jobs": jobs})
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def safe_training_wrapper(job_id, model_id, epochs, lr, batch_size):
    """å®‰å…¨çš„è®­ç»ƒåŒ…è£…å‡½æ•°ï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†"""
    try:
        print(f"ğŸš€ å¯åŠ¨è®­ç»ƒä»»åŠ¡ {job_id}ï¼šæ¨¡å‹ {model_id}, Epochs: {epochs}, LR: {lr}, Batch Size: {batch_size}")
        
        # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
        job_info = None
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                job_info = TRAINING_JOBS[job_id]
                job_info['status'] = 'running'
                job_info['start_time'] = time_module.time()
        
        # æ‰§è¡ŒçœŸå®è®­ç»ƒå¹¶æ•è·å…¶è¿”å›çš„å®Œæ•´ç»“æœ
        results = perform_real_training(job_id, model_id, epochs, lr, batch_size)
        
        # å‡†å¤‡ç”¨äºæŒä¹…åŒ–çš„å†å²è®°å½•æ¡ç›® - å®Œå…¨æŒ‰ç…§å‰ç«¯ui.jsçš„éœ€æ±‚æ¥æ„å»º
        end_time = time_module.time()
        start_time = job_info.get('start_time', end_time)

        history_entry = {
            "job_id": job_id,
            "model_id": model_id,
            "model_name": get_model_name_by_id(model_id),
            "status": "completed",
            "timestamp": datetime.fromtimestamp(end_time).isoformat(), # å‰ç«¯éœ€è¦ timestamp
            "config": job_info.get('config', {}),
            
            # --- ç›´æ¥æš´éœ²åœ¨é¡¶å±‚çš„æŒ‡æ ‡ ---
            "best_accuracy": results.get('best_accuracy', 0.0), # åŸå§‹å‡†ç¡®ç‡ (0.0 to 1.0)
            "final_loss": results.get('final_loss', 0.0),
            "samples_per_second": results.get('samples_per_second', 0),
            "model_params": results.get('model_params', 0),
            "duration_seconds": end_time - start_time, # å‰ç«¯éœ€è¦ duration_seconds

            "epoch_metrics": results.get('epoch_metrics', []),
            "error_message": None
        }

        PERSISTENCE_MANAGER.save_training_history(history_entry)
        
        print(f"âœ… è®­ç»ƒä»»åŠ¡ {job_id} æˆåŠŸå®Œæˆï¼Œç»“æœå·²æŒ‰ç…§å‰ç«¯æ ¼å¼ä¿å­˜ã€‚")
        
    except Exception as e:
        error_message = f"è®­ç»ƒå¤±è´¥: {str(e)}"
        print(f"âŒ è®­ç»ƒä»»åŠ¡ {job_id} å¤±è´¥: {error_message}")
        
        # æ›´æ–°é”™è¯¯çŠ¶æ€
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'error'
                TRAINING_JOBS[job_id]['error_message'] = error_message
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()

def run_training_job(job_id, model_id, epochs, lr, batch_size):
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œçš„è®­ç»ƒä»»åŠ¡ - å·²å¼ƒç”¨ï¼Œä½¿ç”¨ safe_training_wrapper"""
    pass

def is_first_training_for_model(model_id, save_dir):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæŸæ¨¡å‹çš„é¦–æ¬¡è®­ç»ƒ
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆå¦‚'cnn', 'mlp'ç­‰ï¼‰
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•è·¯å¾„
    
    Returns:
        tuple: (is_first_training: bool, historical_best_accuracy: float)
    """
    try:
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)
            return True, 0.0
        
        # æ‰«æå·²æœ‰æ¨¡å‹æ–‡ä»¶
        model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth')]
        
        # æŸ¥æ‰¾ç›¸åŒæ¨¡å‹ç±»å‹çš„æ–‡ä»¶
        matching_files = []
        for filename in model_files:
            if filename.startswith(f"{model_id}_"):
                matching_files.append(filename)
        
        if not matching_files:
            return True, 0.0
        
        # æå–å†å²æœ€ä½³å‡†ç¡®ç‡
        best_accuracy = 0.0
        for filename in matching_files:
            accuracy = extract_accuracy_from_filename(filename)
            if accuracy > best_accuracy:
                best_accuracy = accuracy
        
        return False, best_accuracy
        
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥é¦–æ¬¡è®­ç»ƒçŠ¶æ€å¤±è´¥: {e}")
        return True, 0.0

def extract_accuracy_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
    
    Args:
        filename: æ¨¡å‹æ–‡ä»¶å
    
    Returns:
        float: å‡†ç¡®ç‡ï¼Œè§£æå¤±è´¥æ—¶è¿”å›0.0
    """
    try:
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1. model_id_best_acc_0.9947.pth
        # 2. model_id_20250618_232011_acc_0.9947.pth
        
        if '_acc_' in filename:
            # æ‰¾åˆ°æœ€åä¸€ä¸ª '_acc_'
            last_acc_index = filename.rfind('_acc_')
            accuracy_part = filename[last_acc_index + 5:].replace('.pth', '')
            return float(accuracy_part)
        
        return 0.0
        
    except (ValueError, IndexError):
        return 0.0

def save_model_with_replacement(model, model_id, accuracy, save_dir, job_id):
    """ä¿å­˜æ¨¡å‹ï¼Œå¦‚æœæ€§èƒ½æ›´å¥½åˆ™æ›¿æ¢æ—§æ¨¡å‹
    
    Args:
        model: è¦ä¿å­˜çš„PyTorchæ¨¡å‹
        model_id: æ¨¡å‹ID
        accuracy: å½“å‰æ¨¡å‹çš„å‡†ç¡®ç‡
        save_dir: ä¿å­˜ç›®å½•
        job_id: è®­ç»ƒä»»åŠ¡ID
    """
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è®­ç»ƒ
        is_first, historical_best = is_first_training_for_model(model_id, save_dir)
        
        # æ„å»ºæ–°çš„æ–‡ä»¶åï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
        new_filename = f"{model_id}_best_acc_{accuracy:.4f}.pth"
        new_filepath = os.path.join(save_dir, new_filename)
        
        # å¦‚æœä¸æ˜¯é¦–æ¬¡è®­ç»ƒï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦è¶…è¶Šå†å²æœ€ä½³
        if not is_first:
            if accuracy <= historical_best:
                print(f"âš ï¸ å½“å‰å‡†ç¡®ç‡ {accuracy:.4f} æœªè¶…è¶Šå†å²æœ€ä½³ {historical_best:.4f}")
                return False
            
            # åˆ é™¤æ—§çš„æ¨¡å‹æ–‡ä»¶
            model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth') and f.startswith(f"{model_id}_")]
            for old_file in model_files:
                old_filepath = os.path.join(save_dir, old_file)
                if os.path.exists(old_filepath):
                    os.remove(old_filepath)
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ¨¡å‹æ–‡ä»¶: {old_file}")
        
        # ä¿å­˜æ–°æ¨¡å‹
        torch.save(model.state_dict(), new_filepath)
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜æˆåŠŸ: {new_filename}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(new_filepath) / 1024 / 1024:.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        return False

def find_optimal_batch_size(model, device, base_batch_size=64):
    """åŠ¨æ€å¯»æ‰¾æœ€ä¼˜æ‰¹é‡å¤§å°
    
    Args:
        model: PyTorchæ¨¡å‹
        device: è®¾å¤‡ (cuda/cpu)
        base_batch_size: åŸºç¡€æ‰¹é‡å¤§å°
    
    Returns:
        int: æœ€ä¼˜æ‰¹é‡å¤§å°
    """
    if device.type == 'cpu':
        return min(base_batch_size, 32)  # CPUé™åˆ¶æ›´ä¸¥æ ¼
    
    optimal_batch_size = base_batch_size
    
    try:
        # å°è¯•é€æ­¥å¢åŠ æ‰¹é‡å¤§å°ç›´åˆ°å†…å­˜ä¸è¶³
        for test_batch_size in [64, 128, 256, 512]:
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_input = torch.randn(test_batch_size, 1, 28, 28).to(device)
                
                # å‰å‘ä¼ æ’­æµ‹è¯•
                model.eval()
                with torch.no_grad():
                    _ = model(test_input)
                
                optimal_batch_size = test_batch_size
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    break
                else:
                    raise e
    except Exception as e:
        print(f"âš ï¸ æ‰¹é‡å¤§å°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
    
    return optimal_batch_size

def perform_real_training(job_id, model_id, epochs, lr, batch_size):
    """æ‰§è¡Œå®é™…çš„æ¨¡å‹è®­ç»ƒï¼Œå¹¶åœ¨å®Œæˆåè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸ã€‚"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”© ä½¿ç”¨è®¾å¤‡: {device}")

    # --- è·¯å¾„è®¾ç½® ---
    # ä½¿ç”¨åœ¨æ–‡ä»¶é¡¶éƒ¨å®šä¹‰çš„å…¨å±€è·¯å¾„å¸¸é‡
    os.makedirs(SAVED_MODELS_DIR, exist_ok=True)
    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)
    
    # è·å–å†å²æœ€ä½³å‡†ç¡®ç‡
    is_first_train, historical_best_accuracy = is_first_training_for_model(model_id, SAVED_MODELS_DIR)
    
    with TRAINING_LOCK:
        TRAINING_JOBS[job_id]['progress']['historical_best_accuracy'] = historical_best_accuracy
        TRAINING_JOBS[job_id]['progress']['is_first_training'] = is_first_train

    print(f"ğŸ“š æ¨¡å‹ {model_id} çš„å†å²æœ€ä½³å‡†ç¡®ç‡: {historical_best_accuracy:.4f}")

    train_start_time = time_module.time()
    model = None
    final_accuracy = 0.0
    is_new_record = False
    
    try:
        print(f"ğŸ¯ å¼€å§‹çœŸå®è®­ç»ƒ: {model_id}")
        
        # 1. è®¾å¤‡æ£€æµ‹ã€æ•°æ®åŠ è½½ç­‰...
        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST('data', train=False, transform=transform)
        
        model_instance = get_model_instance(model_id)
        model_instance.to(device)
        optimal_batch_size = find_optimal_batch_size(model_instance, device, batch_size)
        
        train_loader = DataLoader(train_dataset, batch_size=optimal_batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=optimal_batch_size, shuffle=False)
        
        model = get_model_instance(model_id).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr)
        
        best_accuracy = 0.0
        
        # 2. è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            model.train()
            running_loss = 0.0
            for i, (images, labels) in enumerate(train_loader):
                batch_start_time = time_module.time()

                images, labels = images.to(device), labels.to(device)
                
                # å‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­å’Œä¼˜åŒ–
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()

                batch_end_time = time_module.time()
                batch_duration = batch_end_time - batch_start_time
                samples_per_second = len(images) / batch_duration if batch_duration > 0 else 0
                
                # æ›´æ–°å…¨å±€çŠ¶æ€
                with TRAINING_LOCK:
                    if job_id in TRAINING_JOBS:
                        progress_data = TRAINING_JOBS[job_id]['progress']
                        progress_data['percentage'] = min(100, int(((epoch * len(train_loader) + i + 1) / (epochs * len(train_loader))) * 100))
                        progress_data['current_epoch'] = epoch + 1
                        progress_data['loss'] = loss.item()
                        progress_data['samples_per_second'] = samples_per_second
            
            # ... (æµ‹è¯•é˜¶æ®µä»£ç ä¿æŒä¸å˜) ...
            model.eval()
            test_loss = 0.0
            correct = 0
            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    test_loss += criterion(output, target).item()
                    pred = output.argmax(dim=1, keepdim=True)
                    correct += pred.eq(target.view_as(pred)).sum().item()
            
            test_accuracy = correct / len(test_loader.dataset)
            if test_accuracy > best_accuracy:
                best_accuracy = test_accuracy
                is_new_record = PERSISTENCE_MANAGER.save_checkpoint(model, model_id, job_id, epoch, best_accuracy)

            with TRAINING_LOCK:
                 if job_id in TRAINING_JOBS:
                    TRAINING_JOBS[job_id]['progress'].update({
                        'percentage': ((epoch + 1) / epochs) * 100,
                        'current_epoch': epoch + 1,
                        'accuracy': test_accuracy,
                        'best_accuracy': best_accuracy,
                    })

        final_accuracy = best_accuracy
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'completed'
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()
        
    except Exception as e:
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'error'
                TRAINING_JOBS[job_id]['error_message'] = str(e)
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()
    
    finally:
        # 3. æ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½è®°å½•åˆ°å†å²æ¡£æ¡ˆ
        training_duration = time_module.time() - train_start_time
        history_entry = {
            "job_id": job_id,
            "model_id": model_id,
            "model_name": get_model_name_by_id(model_id),
            "has_attention": get_model_display_info(model_id).get('has_attention', False),
            "status": TRAINING_JOBS.get(job_id, {}).get('status', 'unknown'),
            "start_time": train_start_time,
            "completion_time": time_module.time(),
            "hyperparameters": {
                "epochs": epochs,
                "learning_rate": lr,
                "batch_size": batch_size
            },
            "metrics": {
                "final_accuracy": final_accuracy,
                "training_duration_sec": training_duration,
                "is_new_record": is_new_record,
                "total_params": sum(p.numel() for p in model.parameters()) if model else 0
            },
            "error_message": TRAINING_JOBS.get(job_id, {}).get('error_message')
        }
        PERSISTENCE_MANAGER.append_training_history(history_entry)
        print(f"Hï¸âƒ£ è®­ç»ƒå†å²å·²å­˜æ¡£: {job_id}")

        # åœ¨æ‰€æœ‰epochç»“æŸå
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if best_accuracy > historical_best_accuracy:
            print(f"ğŸ‰ æ–°çºªå½•! å‡†ç¡®ç‡ä» {historical_best_accuracy:.4f} æå‡åˆ° {best_accuracy:.4f}ã€‚æ­£åœ¨ä¿å­˜æ¨¡å‹...")
            save_model_with_replacement(model, model_id, best_accuracy, SAVED_MODELS_DIR, job_id)
        else:
            print(f"ğŸ‘ è®­ç»ƒå®Œæˆï¼Œä½†æœªè¶…è¶Šå†å²æœ€ä½³å‡†ç¡®ç‡({historical_best_accuracy:.4f})")

        # è¿”å›åŒ…å«æ‰€æœ‰éœ€è¦ä¿¡æ¯çš„å­—å…¸
        return {
            "best_accuracy": round(best_accuracy, 4),
            "final_loss": round(test_loss, 5),
            "total_time": training_duration,
            "samples_per_second": samples_per_second,
            "model_params": sum(p.numel() for p in model.parameters()) if model else 0
        }

@app.route('/api/history', methods=['GET'])
def get_training_history():
    """è·å–æ‰€æœ‰å·²å®Œæˆçš„è®­ç»ƒå†å²è®°å½•"""
    try:
        history = PERSISTENCE_MANAGER.load_training_history()
        # åœ¨è¿”å›ä¹‹å‰ï¼Œä¸ºæ¯æ¡è®°å½•æ·»åŠ  model_name
        for record in history:
            record['model_name'] = get_model_name_by_id(record['model_id'])
        return jsonify(history)
    except Exception as e:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if isinstance(e, FileNotFoundError):
            return jsonify([])
        print(f"âŒ è·å–è®­ç»ƒå†å²å¤±è´¥: {e}")
        return jsonify({"error": "æ— æ³•è·å–è®­ç»ƒå†å²"}), 500

@app.route('/api/training_progress', methods=['GET'])
def get_training_progress():
    """è·å–è®­ç»ƒè¿›åº¦"""
    try:
        job_ids = request.args.get('job_ids')
        if not job_ids:
            return jsonify({"error": "ç¼ºå°‘job_idså‚æ•°"}), 400
        
        job_list = [job_id.strip() for job_id in job_ids.split(',')]
        
        progress_list = []
        
        with TRAINING_LOCK:
            for job_id in job_list:
                if job_id in TRAINING_JOBS:
                    job_info = TRAINING_JOBS[job_id].copy()
                    progress_list.append(job_info)
                else:
                    return jsonify({"error": f"è®­ç»ƒä»»åŠ¡ {job_id} ä¸å­˜åœ¨"}), 404
        
        # è¿”å›å‰ç«¯æœŸæœ›çš„æ ¼å¼
        return jsonify({"progress": progress_list})
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/trained_models', methods=['GET'])
def get_trained_models():
    """è·å–å·²è®­ç»ƒæ¨¡å‹åˆ—è¡¨"""
    try:
        trained_models = scan_trained_models()
        return jsonify(trained_models)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def scan_trained_models():
    """æ‰«ææ¨¡å‹ä¿å­˜ç›®å½•ï¼Œè¿”å›å¯ç”¨äºé¢„æµ‹çš„æ¨¡å‹åˆ—è¡¨"""
    
    if not os.path.exists(SAVED_MODELS_DIR):
        print(f"âš ï¸ æ¨¡å‹ç›®å½• {SAVED_MODELS_DIR} ä¸å­˜åœ¨ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚")
        return []
        
    models = []
    # æ­£åˆ™è¡¨è¾¾å¼æ”¹è¿›ï¼šæ›´ç²¾ç¡®åœ°åŒ¹é…æ¨¡å‹IDã€å‡†ç¡®ç‡å’Œå¯é€‰çš„job_id
    # åˆ†ç»„: (1:model_id) (2:accuracy) (3:job_id, a non-capturing group for the 'job' prefix)
    pattern = re.compile(r"^(.*?_attention|cnn|mlp|rnn)_(\d+\.\d{2})%_acc(?:_job_.*)?\.pth$")

    for filename in os.listdir(SAVED_MODELS_DIR):
        match = pattern.match(filename)
        if match:
            model_id = match.group(1)
            accuracy_str = match.group(2)
            
            # ä½¿ç”¨æ›´ç¨³å¥çš„æ–¹å¼æŸ¥æ‰¾æ¨¡å‹æ˜¾ç¤ºåç§°å’Œå‚æ•°
            display_info = get_model_display_info(model_id)
            
            models.append({
                "model_id": model_id,
                "display_name": display_info["name"],
                "accuracy": float(accuracy_str),
                "parameter_count": display_info["parameter_count"],
                "filename": filename
            })

    # æŒ‰å‡†ç¡®ç‡é™åºæ’åº
    models.sort(key=lambda x: x['accuracy'], reverse=True)
    return models

def get_model_display_info(model_id):
    """æ ¹æ®æ¨¡å‹IDè·å–å…¶æ˜¾ç¤ºåç§°å’Œå‚æ•°æ•°é‡"""
    model_config = next((m for m in AVAILABLE_MODELS if m['id'] == model_id), None)
    if model_config:
        return {
            "name": model_config["name"],
            "parameter_count": model_config.get("parameter_count", "N/A")
        }
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œæä¾›ä¸€ä¸ªä¼˜é›…çš„é™çº§æ–¹æ¡ˆ
    return {
        "name": model_id.replace("_", " ").title(),
        "parameter_count": "N/A"
    }

@app.route('/api/predict', methods=['POST'])
def predict():
    """å¤„ç†æ‰‹å†™æ•°å­—è¯†åˆ«è¯·æ±‚"""
    try:
        data = request.get_json()
        if not data or 'model_id' not in data or 'image_base64' not in data:
            return jsonify({"error": "è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ï¼Œéœ€è¦åŒ…å«model_idå’Œimage_base64å­—æ®µ"}), 400
        
        model_id = data['model_id']
        image_base64 = data['image_base64']
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if model_id not in LOADED_MODELS:
            print(f"â³ é¦–æ¬¡é¢„æµ‹è¯·æ±‚ï¼Œæ­£åœ¨åŠ è½½æ¨¡å‹: {model_id}")
            model = load_model_for_prediction(model_id)
            if model is None:
                return jsonify({"error": f"æ¨¡å‹æ–‡ä»¶ '{model_id}' ä¸å­˜åœ¨æˆ–æ— æ³•åŠ è½½"}), 404
            LOADED_MODELS[model_id] = model
        
        model = LOADED_MODELS[model_id]
        
        # é¢„å¤„ç†å›¾åƒ
        input_tensor = preprocess_canvas_image(image_base64)
        
        # æ‰§è¡Œæ¨æ–­
        prediction, probabilities = perform_inference(model, input_tensor)
        
        # è¿”å›ç»“æœ
        return jsonify({
            "prediction": prediction,
            "probabilities": probabilities.tolist()
        })
    except Exception as e:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¯ä»¥è€ƒè™‘è®°å½•æ›´è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
        return jsonify({"error": "é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿå†…éƒ¨é”™è¯¯"}), 500

def load_model_for_prediction(model_id):
    """ä¸ºé¢„æµ‹åŠ è½½æŒ‡å®šçš„ã€æœ€æ–°çš„æ¨¡å‹"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if model_id in LOADED_MODELS:
            print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹: {model_id}")
            return LOADED_MODELS[model_id]
        
        # ä½¿ç”¨å…¨å±€è·¯å¾„å¸¸é‡æ„å»ºæ¨¡å‹æ–‡ä»¶è·¯å¾„
        model_path = os.path.join(SAVED_MODELS_DIR, f"{model_id}.pth")
        
        if not os.path.exists(model_path):
            print(f"âŒ é¢„æµ‹æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
            
        print(f"ê°™ì€ ëª¨ë¸ ë¡œë”©: {model_path}")

        # --- æ™ºèƒ½è§£ææ¨¡å‹ç±»å‹ (Smartly Parse Model Type) ---
        # ä»å®Œæ•´æ–‡ä»¶åä¸­æå–åŸºç¡€æ¨¡å‹IDï¼Œä¾‹å¦‚ä» 'cnn_attention_best_acc_0.9947' æå– 'cnn_attention'
        delimiter_pattern = r'_(?:best_)?acc_'
        parts = re.split(delimiter_pattern, model_id)
        model_type = parts[0]

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: '{model_type}' (ä» '{model_id}' è§£æ)")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model_instance = get_model_instance(model_type)
        
        # åŠ è½½æƒé‡
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            model_instance.load_state_dict(checkpoint)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            model_instance.eval()
            
            # ç¼“å­˜æ¨¡å‹ï¼ˆé™åˆ¶ç¼“å­˜æ•°é‡é¿å…å†…å­˜è¿‡å¤šï¼‰
            if len(LOADED_MODELS) >= 5:  # æœ€å¤šç¼“å­˜5ä¸ªæ¨¡å‹
                # åˆ é™¤æœ€æ—§çš„æ¨¡å‹
                oldest_key = next(iter(LOADED_MODELS))
                del LOADED_MODELS[oldest_key]
                print(f"ğŸ—‘ï¸ åˆ é™¤ç¼“å­˜æ¨¡å‹: {oldest_key}")
            
            LOADED_MODELS[model_id] = model_instance
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸå¹¶ç¼“å­˜: {model_id}")
            
            return model_instance
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
            return None
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_id}: {e}")
        return None

def preprocess_canvas_image(image_base64):
    """é¢„å¤„ç†Canvaså›¾åƒæ•°æ®
    
    Args:
        image_base64: base64ç¼–ç çš„å›¾åƒæ•°æ®
    
    Returns:
        torch.Tensor: é¢„å¤„ç†åçš„tensorï¼Œå½¢çŠ¶ä¸º(1, 1, 28, 28)ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # è§£ç base64å›¾åƒ
        if image_base64.startswith('data:image/'):
            # ç§»é™¤data URLå‰ç¼€
            image_base64 = image_base64.split(',')[1]
        
        # base64è§£ç 
        image_data = base64.b64decode(image_base64)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        image = Image.open(io.BytesIO(image_data))
        
        # ç¡®ä¿å›¾åƒæ˜¯ç°åº¦çš„
        image = image.convert('L')
        
        # è°ƒæ•´å¤§å°åˆ° 28x28
        image = image.resize((28, 28), Image.Resampling.LANCZOS)
        
        # è½¬æ¢ä¸ºPyTorch Tensor
        # æ ‡å‡†åŒ–å‚æ•°å¿…é¡»å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        tensor = transform(image)
        
        # æ·»åŠ ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦ (C, H, W) -> (B, C, H, W)
        return tensor.unsqueeze(0)

    except Exception as e:
        # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯
        import traceback
        traceback.print_exc()
        raise ValueError(f"æ— æ³•å¤„ç†base64å›¾åƒæ•°æ®: {e}")

def perform_inference(model, input_tensor):
    """æ‰§è¡Œæ¨¡å‹æ¨ç†
    
    Args:
        model: PyTorchæ¨¡å‹
        input_tensor: è¾“å…¥tensor
    
    Returns:
        tuple: (prediction, probabilities) - é¢„æµ‹ç»“æœå’Œæ¦‚ç‡åˆ†å¸ƒ
    """
    try:
        with torch.no_grad():
            # å‰å‘ä¼ æ’­
            outputs = model(input_tensor)
            
            # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
            probabilities = torch.softmax(outputs, dim=1)
            
            # è·å–é¢„æµ‹ç»“æœ
            prediction = torch.argmax(probabilities, dim=1)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            probabilities = probabilities.squeeze().numpy()
            prediction = prediction.item()
            
            return prediction, probabilities
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        raise e

def check_system_health():
    """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        health_info = {
            "status": "healthy",
            "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S"),
            "active_training_jobs": len([job for job in TRAINING_JOBS.values() if job['status'] == 'running']),
            "loaded_models": len(LOADED_MODELS),
            "available_models": len(AVAILABLE_MODELS)
        }
        
        return health_info
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S")
        }

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨MNISTæ™ºèƒ½åˆ†æå¹³å°åç«¯æœåŠ¡...")
    print("ğŸ”§ ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
    
    health = check_system_health()
    print(f"âœ… ç³»ç»ŸçŠ¶æ€: {health['status']}")
    print(f"ğŸ“Š å¯ç”¨æ¨¡å‹: {health['available_models']} ä¸ª")
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(host='0.0.0.0', port=5000, debug=True) 