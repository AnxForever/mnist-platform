# Flask åº”ç”¨ä¸»å…¥å£æ–‡ä»¶
# å®šä¹‰æ‰€æœ‰ API è·¯ç”±

from flask import Flask, request, jsonify, Response
from flask_cors import CORS
import os
import uuid
import time as time_module  # ä½¿ç”¨åˆ«åé¿å…æ½œåœ¨çš„å‘½åå†²çª
import threading
from concurrent.futures import ThreadPoolExecutor
import base64
import io
import numpy as np
import json

# PyTorch and related imports
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# PIL for image processing
try:
    from PIL import Image
except ImportError:
    print("âš ï¸ PILåº“æœªå®‰è£…ï¼Œæ‰‹å†™è¯†åˆ«åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
    Image = None

# Project-specific imports
from models import get_model_instance

app = Flask(__name__)
CORS(app)

# é…ç½®Flaskåº”ç”¨ä»¥æ­£ç¡®å¤„ç†ä¸­æ–‡JSONè¾“å‡º
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True

# å…¨å±€çŠ¶æ€ç®¡ç†
TRAINING_JOBS = {}
LOADED_MODELS = {}
TRAINING_EXECUTOR = ThreadPoolExecutor(max_workers=3)
TRAINING_LOCK = threading.Lock()

# 6ä¸ªæ¨¡å‹çš„é…ç½®ä¿¡æ¯
AVAILABLE_MODELS = [
    {
        "id": "mlp",
        "name": "MLP (å¤šå±‚æ„ŸçŸ¥æœº)",
        "description": "æœ€ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ",
        "has_attention": False,
        "parameter_count": 79510
    },
    {
        "id": "cnn",
        "name": "CNN (å·ç§¯ç¥ç»ç½‘ç»œ)",
        "description": "ä¸“ä¸ºå›¾åƒå¤„ç†è®¾è®¡çš„ç»å…¸æ¶æ„",
        "has_attention": False,
        "parameter_count": 94214
    },
    {
        "id": "rnn",
        "name": "RNN (å¾ªç¯ç¥ç»ç½‘ç»œ)",
        "description": "å°†å›¾åƒæŒ‰è¡Œåºåˆ—åŒ–å¤„ç†çš„å®éªŒæ€§æ–¹æ³•",
        "has_attention": False,
        "parameter_count": 127626
    },
    {
        "id": "mlp_attention",
        "name": "MLP + Attention",
        "description": "åœ¨MLPåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶",
        "has_attention": True,
        "parameter_count": 85638
    },
    {
        "id": "cnn_attention",
        "name": "CNN + Attention",
        "description": "åœ¨CNNåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°ç©ºé—´æ³¨æ„åŠ›",
        "has_attention": True,
        "parameter_count": 102350
    },
    {
        "id": "rnn_attention",
        "name": "RNN + Attention",
        "description": "åœ¨RNNåŸºç¡€ä¸ŠåŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºåºåˆ—å»ºæ¨¡èƒ½åŠ›",
        "has_attention": True,
        "parameter_count": 134792
    }
]

@app.route('/api/status', methods=['GET'])
def get_status():
    """æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€"""
    return jsonify({
        "status": "running",
        "message": "MNISTæ™ºèƒ½åˆ†æå¹³å°åç«¯æœåŠ¡æ­£å¸¸è¿è¡Œ",
        "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S")
    })

@app.route('/api/models', methods=['GET'])
def get_models():
    """è·å–å¯é€‰æ¨¡å‹åˆ—è¡¨"""
    try:
        # ä½¿ç”¨json.dumpsç¡®ä¿ä¸­æ–‡å­—ç¬¦æ­£ç¡®è¾“å‡º
        json_str = json.dumps(AVAILABLE_MODELS, ensure_ascii=False, indent=2)
        return Response(json_str, mimetype='application/json; charset=utf-8')
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/train', methods=['POST'])
def start_training():
    """å¯åŠ¨æ¨¡å‹è®­ç»ƒ"""
    try:
        data = request.get_json()
        if not data or 'models' not in data:
            return jsonify({"error": "è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ï¼Œéœ€è¦åŒ…å«modelså­—æ®µ"}), 400
        
        training_configs = data['models']
        jobs = []
        
        for config in training_configs:
            # ç”Ÿæˆå”¯ä¸€çš„job_id
            job_id = f"job_{config['id']}_{int(time_module.time() * 1000)}"
            
            # åˆ›å»ºä»»åŠ¡ä¿¡æ¯
            job_info = {
                "job_id": job_id,
                "model_id": config['id'],
                "status": "queued",
                "start_time": time_module.time(),
                "end_time": None,
                "progress": {
                    "percentage": 0,
                    "current_epoch": 0,
                    "total_epochs": config.get('epochs', 10),
                    "accuracy": 0.0,
                    "loss": 0.0,
                    "best_accuracy": 0.0
                },
                "config": {
                    "epochs": config.get('epochs', 10),
                    "learning_rate": config.get('lr', 0.001),
                    "batch_size": config.get('batch_size', 64)
                },
                "error_message": None
            }
            
            # ä¿å­˜åˆ°å…¨å±€çŠ¶æ€
            TRAINING_JOBS[job_id] = job_info
            
            jobs.append({
                "job_id": job_id,
                "model_id": config['id']
            })
            
            # å¯åŠ¨å®é™…çš„è®­ç»ƒä»»åŠ¡
            TRAINING_EXECUTOR.submit(safe_training_wrapper, job_id, config['id'], config.get('epochs', 10), config.get('lr', 0.001), config.get('batch_size', 64))
        
        return jsonify({"jobs": jobs})
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def safe_training_wrapper(job_id, model_id, epochs, lr, batch_size):
    """å®‰å…¨çš„è®­ç»ƒåŒ…è£…å‡½æ•°ï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†"""
    try:
        print(f"ğŸš€ å¯åŠ¨è®­ç»ƒä»»åŠ¡ {job_id}ï¼šæ¨¡å‹ {model_id}, Epochs: {epochs}, LR: {lr}, Batch Size: {batch_size}")
        
        # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'running'
                TRAINING_JOBS[job_id]['start_time'] = time_module.time()
        
        # æ‰§è¡ŒçœŸå®è®­ç»ƒ
        perform_real_training(job_id, model_id, epochs, lr, batch_size)
        
        print(f"âœ… è®­ç»ƒä»»åŠ¡ {job_id} æˆåŠŸå®Œæˆ")
        
    except Exception as e:
        error_message = f"è®­ç»ƒå¤±è´¥: {str(e)}"
        print(f"âŒ è®­ç»ƒä»»åŠ¡ {job_id} å¤±è´¥: {error_message}")
        
        # æ›´æ–°é”™è¯¯çŠ¶æ€
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'error'
                TRAINING_JOBS[job_id]['error_message'] = error_message
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()

def run_training_job(job_id, model_id, epochs, lr, batch_size):
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œçš„è®­ç»ƒä»»åŠ¡ - å·²å¼ƒç”¨ï¼Œä½¿ç”¨ safe_training_wrapper"""
    pass

def is_first_training_for_model(model_id, save_dir):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæŸæ¨¡å‹çš„é¦–æ¬¡è®­ç»ƒ
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆå¦‚'cnn', 'mlp'ç­‰ï¼‰
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•è·¯å¾„
    
    Returns:
        tuple: (is_first_training: bool, historical_best_accuracy: float)
    """
    try:
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)
            return True, 0.0
        
        # æ‰«æå·²æœ‰æ¨¡å‹æ–‡ä»¶
        model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth')]
        
        # æŸ¥æ‰¾ç›¸åŒæ¨¡å‹ç±»å‹çš„æ–‡ä»¶
        matching_files = []
        for filename in model_files:
            if filename.startswith(f"{model_id}_"):
                matching_files.append(filename)
        
        if not matching_files:
            return True, 0.0
        
        # æå–å†å²æœ€ä½³å‡†ç¡®ç‡
        best_accuracy = 0.0
        for filename in matching_files:
            accuracy = extract_accuracy_from_filename(filename)
            if accuracy > best_accuracy:
                best_accuracy = accuracy
        
        return False, best_accuracy
        
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥é¦–æ¬¡è®­ç»ƒçŠ¶æ€å¤±è´¥: {e}")
        return True, 0.0

def extract_accuracy_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–å‡†ç¡®ç‡
    
    Args:
        filename: æ¨¡å‹æ–‡ä»¶å
    
    Returns:
        float: å‡†ç¡®ç‡ï¼Œè§£æå¤±è´¥æ—¶è¿”å›0.0
    """
    try:
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1. model_id_best_acc_0.9947.pth
        # 2. model_id_20250618_232011_acc_0.9947.pth
        
        if '_acc_' in filename:
            # æ‰¾åˆ°æœ€åä¸€ä¸ª '_acc_'
            last_acc_index = filename.rfind('_acc_')
            accuracy_part = filename[last_acc_index + 5:].replace('.pth', '')
            return float(accuracy_part)
        
        return 0.0
        
    except (ValueError, IndexError):
        return 0.0

def save_model_with_replacement(model, model_id, accuracy, save_dir, job_id):
    """ä¿å­˜æ¨¡å‹ï¼Œå¦‚æœæ€§èƒ½æ›´å¥½åˆ™æ›¿æ¢æ—§æ¨¡å‹
    
    Args:
        model: è¦ä¿å­˜çš„PyTorchæ¨¡å‹
        model_id: æ¨¡å‹ID
        accuracy: å½“å‰æ¨¡å‹çš„å‡†ç¡®ç‡
        save_dir: ä¿å­˜ç›®å½•
        job_id: è®­ç»ƒä»»åŠ¡ID
    """
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è®­ç»ƒ
        is_first, historical_best = is_first_training_for_model(model_id, save_dir)
        
        # æ„å»ºæ–°çš„æ–‡ä»¶åï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
        new_filename = f"{model_id}_best_acc_{accuracy:.4f}.pth"
        new_filepath = os.path.join(save_dir, new_filename)
        
        # å¦‚æœä¸æ˜¯é¦–æ¬¡è®­ç»ƒï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦è¶…è¶Šå†å²æœ€ä½³
        if not is_first:
            if accuracy <= historical_best:
                print(f"âš ï¸ å½“å‰å‡†ç¡®ç‡ {accuracy:.4f} æœªè¶…è¶Šå†å²æœ€ä½³ {historical_best:.4f}")
                return False
            
            # åˆ é™¤æ—§çš„æ¨¡å‹æ–‡ä»¶
            model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth') and f.startswith(f"{model_id}_")]
            for old_file in model_files:
                old_filepath = os.path.join(save_dir, old_file)
                if os.path.exists(old_filepath):
                    os.remove(old_filepath)
                    print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ¨¡å‹æ–‡ä»¶: {old_file}")
        
        # ä¿å­˜æ–°æ¨¡å‹
        torch.save(model.state_dict(), new_filepath)
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜æˆåŠŸ: {new_filename}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(new_filepath) / 1024 / 1024:.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        return False

def find_optimal_batch_size(model, device, base_batch_size=64):
    """åŠ¨æ€å¯»æ‰¾æœ€ä¼˜æ‰¹é‡å¤§å°
    
    Args:
        model: PyTorchæ¨¡å‹
        device: è®¾å¤‡ (cuda/cpu)
        base_batch_size: åŸºç¡€æ‰¹é‡å¤§å°
    
    Returns:
        int: æœ€ä¼˜æ‰¹é‡å¤§å°
    """
    if device.type == 'cpu':
        return min(base_batch_size, 32)  # CPUé™åˆ¶æ›´ä¸¥æ ¼
    
    optimal_batch_size = base_batch_size
    
    try:
        # å°è¯•é€æ­¥å¢åŠ æ‰¹é‡å¤§å°ç›´åˆ°å†…å­˜ä¸è¶³
        for test_batch_size in [64, 128, 256, 512]:
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_input = torch.randn(test_batch_size, 1, 28, 28).to(device)
                
                # å‰å‘ä¼ æ’­æµ‹è¯•
                model.eval()
                with torch.no_grad():
                    _ = model(test_input)
                
                optimal_batch_size = test_batch_size
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    break
                else:
                    raise e
    except Exception as e:
        print(f"âš ï¸ æ‰¹é‡å¤§å°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
    
    return optimal_batch_size

def perform_real_training(job_id, model_id, epochs, lr, batch_size):
    """æ‰§è¡ŒçœŸå®çš„æ¨¡å‹è®­ç»ƒ
    
    Args:
        job_id: è®­ç»ƒä»»åŠ¡ID
        model_id: æ¨¡å‹ID
        epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
        batch_size: æ‰¹é‡å¤§å°
    """
    try:
        print(f"ğŸ¯ å¼€å§‹çœŸå®è®­ç»ƒ: {model_id}")
        
        # 1. è®¾å¤‡æ£€æµ‹
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # 2. æ•°æ®åŠ è½½
        print("ğŸ“Š åŠ è½½MNISTæ•°æ®é›†...")
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)
        test_dataset = datasets.MNIST('data', train=False, transform=transform)
        
        # åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°
        model_instance = get_model_instance(model_id)
        model_instance.to(device)
        
        optimal_batch_size = find_optimal_batch_size(model_instance, device, batch_size)
        print(f"ğŸ“¦ ä¼˜åŒ–åæ‰¹é‡å¤§å°: {optimal_batch_size}")
        
        train_loader = DataLoader(train_dataset, batch_size=optimal_batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=optimal_batch_size, shuffle=False)
        
        # 3. æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        model = get_model_instance(model_id).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr)
        
        print(f"ğŸ§  æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        # 4. è®­ç»ƒå¾ªç¯
        best_accuracy = 0.0
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        save_dir = 'backend/saved_models'
        os.makedirs(save_dir, exist_ok=True)
        
        for epoch in range(epochs):
            epoch_start_time = time_module.time()
            
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            correct_train = 0
            total_train = 0
            
            epoch_start = time_module.time()
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total_train += target.size(0)
                correct_train += (predicted == target).sum().item()
                
                # æ›´æ–°è¿›åº¦
                if batch_idx % 100 == 0:
                    progress_percentage = (epoch * len(train_loader) + batch_idx) / (epochs * len(train_loader)) * 100
                    
                    with TRAINING_LOCK:
                        if job_id in TRAINING_JOBS:
                            TRAINING_JOBS[job_id]['progress'].update({
                                'percentage': progress_percentage,
                                'current_epoch': epoch + 1,
                                'loss': loss.item(),
                                'accuracy': correct_train / total_train
                            })
            
            # æµ‹è¯•é˜¶æ®µ
            model.eval()
            test_loss = 0.0
            correct_test = 0
            total_test = 0
            
            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    test_loss += criterion(output, target).item()
                    _, predicted = torch.max(output.data, 1)
                    total_test += target.size(0)
                    correct_test += (predicted == target).sum().item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            train_accuracy = correct_train / total_train
            test_accuracy = correct_test / total_test
            epoch_time = time_module.time() - epoch_start
            
            print(f"ğŸ“ˆ Epoch {epoch+1}/{epochs}:")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss/len(train_loader):.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
            print(f"   æµ‹è¯•æŸå¤±: {test_loss/len(test_loader):.4f}, æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
            print(f"   è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡å¹¶ä¿å­˜æ¨¡å‹
            if test_accuracy > best_accuracy:
                best_accuracy = test_accuracy
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                save_success = save_model_with_replacement(model, model_id, test_accuracy, save_dir, job_id)
                if save_success:
                    print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å‡†ç¡®ç‡: {test_accuracy:.4f}")
            
            # æ›´æ–°è®­ç»ƒè¿›åº¦
            with TRAINING_LOCK:
                if job_id in TRAINING_JOBS:
                    TRAINING_JOBS[job_id]['progress'].update({
                        'percentage': ((epoch + 1) / epochs) * 100,
                        'current_epoch': epoch + 1,
                        'accuracy': test_accuracy,
                        'loss': test_loss / len(test_loader),
                        'best_accuracy': best_accuracy
                    })
        
        # è®­ç»ƒå®Œæˆ
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'completed'
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()
                TRAINING_JOBS[job_id]['progress']['percentage'] = 100
        
        print(f"ğŸŠ è®­ç»ƒå®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
        
    except Exception as e:
        error_message = f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {str(e)}"
        print(f"âŒ {error_message}")
        
        # æ›´æ–°é”™è¯¯çŠ¶æ€
        with TRAINING_LOCK:
            if job_id in TRAINING_JOBS:
                TRAINING_JOBS[job_id]['status'] = 'error'
                TRAINING_JOBS[job_id]['error_message'] = error_message
                TRAINING_JOBS[job_id]['end_time'] = time_module.time()
        
        raise e

@app.route('/api/training_progress', methods=['GET'])
def get_training_progress():
    """è·å–è®­ç»ƒè¿›åº¦"""
    try:
        job_ids = request.args.get('job_ids')
        if not job_ids:
            return jsonify({"error": "ç¼ºå°‘job_idså‚æ•°"}), 400
        
        job_list = [job_id.strip() for job_id in job_ids.split(',')]
        
        progress_data = {}
        
        with TRAINING_LOCK:
            for job_id in job_list:
                if job_id in TRAINING_JOBS:
                    job_info = TRAINING_JOBS[job_id].copy()
                    progress_data[job_id] = job_info
                else:
                    return jsonify({"error": f"è®­ç»ƒä»»åŠ¡ {job_id} ä¸å­˜åœ¨"}), 404
        
        return jsonify(progress_data)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/trained_models', methods=['GET'])
def get_trained_models():
    """è·å–å·²è®­ç»ƒæ¨¡å‹åˆ—è¡¨"""
    try:
        trained_models = scan_trained_models()
        return jsonify(trained_models)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def scan_trained_models():
    """æ‰«æsaved_modelsç›®å½•ä¸­çš„å·²è®­ç»ƒæ¨¡å‹
    
    Returns:
        list: å·²è®­ç»ƒæ¨¡å‹åˆ—è¡¨ï¼Œæ¯ä¸ªæ¨¡å‹åŒ…å«idã€nameã€model_typeã€accuracyç­‰ä¿¡æ¯
    """
    # ä¼˜å…ˆä½¿ç”¨backend/saved_modelsç›®å½•
    if os.path.exists('backend/saved_models'):
        save_dir = 'backend/saved_models'
    elif os.path.exists('saved_models'):
        save_dir = 'saved_models'
    else:
        save_dir = 'backend/saved_models'  # å›é€€åˆ°æ ‡å‡†ä½ç½®
    trained_models = []
    
    try:
        if not os.path.exists(save_dir):
            print(f"ğŸ“ æ¨¡å‹ä¿å­˜ç›®å½•ä¸å­˜åœ¨: {save_dir}")
            return trained_models
        
        # è·å–ç›®å½•ä¸­çš„æ‰€æœ‰ .pth æ–‡ä»¶
        model_files = [f for f in os.listdir(save_dir) if f.endswith('.pth')]
        
        if not model_files:
            print(f"ğŸ“‚ æ¨¡å‹ä¿å­˜ç›®å½•ä¸ºç©º: {save_dir}")
            return trained_models
        
        print(f"ğŸ” æ‰«æåˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")
        
        for filename in model_files:
            try:
                # è§£ææ–‡ä»¶åæ ¼å¼: {model_id}_best_acc_{accuracy:.4f}.pth
                if '_best_acc_' not in filename:
                    continue
                    
                # æ‰¾åˆ°æœ€åä¸€ä¸ª '_best_acc_' æ¥æ­£ç¡®åˆ†å‰²æ–‡ä»¶å
                last_acc_index = filename.rfind('_best_acc_')
                if last_acc_index == -1:
                    continue
                
                model_id = filename[:last_acc_index]
                accuracy_part = filename[last_acc_index + len('_best_acc_'):].replace('.pth', '')
                accuracy = float(accuracy_part)
                
                # è·å–æ¨¡å‹æ˜¾ç¤ºåç§°å’Œä¿¡æ¯
                model_info = get_model_display_info(model_id)
                if not model_info:
                    continue
                
                # è·å–æ–‡ä»¶ä¿¡æ¯
                file_path = os.path.join(save_dir, filename)
                file_stats = os.stat(file_path)
                file_size = file_stats.st_size
                
                # æ ¼å¼åŒ–è®­ç»ƒæ—¶é—´ï¼ˆä½¿ç”¨time_moduleé¿å…å‘½åå†²çªï¼‰
                mtime = file_stats.st_mtime
                local_time = time_module.localtime(mtime)
                training_time = time_module.strftime('%Y-%m-%dT%H:%M:%S', local_time)
                
                trained_model = {
                    "id": filename.replace('.pth', ''),  # ä½¿ç”¨å®Œæ•´æ–‡ä»¶åä½œä¸ºID
                    "name": f"{model_info['name']} (å‡†ç¡®ç‡: {accuracy:.2%})",
                    "model_type": model_id,
                    "has_attention": model_info['has_attention'],
                    "accuracy": accuracy,
                    "training_time": training_time,
                    "file_size": file_size,
                    "parameter_count": model_info['parameter_count']
                }
                
                trained_models.append(trained_model)
                print(f"âœ… å‘ç°æ¨¡å‹: {model_id} - å‡†ç¡®ç‡: {accuracy:.4f}")
                
            except Exception as e:
                print(f"âš ï¸ è§£ææ¨¡å‹æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                continue
        
        # æŒ‰å‡†ç¡®ç‡é™åºæ’åº
        trained_models.sort(key=lambda x: x['accuracy'], reverse=True)
        
        print(f"ğŸ“‹ æˆåŠŸæ‰«æ {len(trained_models)} ä¸ªæœ‰æ•ˆçš„å·²è®­ç»ƒæ¨¡å‹")
        return trained_models
        
    except Exception as e:
        print(f"âŒ æ‰«æå·²è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")
        return []

def get_model_display_info(model_id):
    """è·å–æ¨¡å‹çš„æ˜¾ç¤ºä¿¡æ¯
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆå¦‚'cnn', 'mlp'ç­‰ï¼‰
    
    Returns:
        dict: æ¨¡å‹æ˜¾ç¤ºä¿¡æ¯ï¼Œå¦‚æœæ¨¡å‹IDæ— æ•ˆåˆ™è¿”å›None
    """
    # åœ¨å…¨å±€æ¨¡å‹é…ç½®ä¸­æŸ¥æ‰¾
    for model_config in AVAILABLE_MODELS:
        if model_config['id'] == model_id:
            return {
                'name': model_config['name'],
                'has_attention': model_config['has_attention'],
                'parameter_count': model_config['parameter_count']
            }
    
    print(f"âš ï¸ æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_id}")
    return None

@app.route('/api/predict', methods=['POST'])
def predict():
    """æ‰§è¡Œæ‰‹å†™è¯†åˆ«"""
    try:
        data = request.get_json()
        if not data or 'model_id' not in data or 'image_base64' not in data:
            return jsonify({"error": "è¯·æ±‚ä½“æ ¼å¼é”™è¯¯ï¼Œéœ€è¦åŒ…å«model_idå’Œimage_base64å­—æ®µ"}), 400
        
        model_id = data['model_id']
        image_base64 = data['image_base64']
        
        print(f"ğŸ” å¼€å§‹é¢„æµ‹ï¼Œæ¨¡å‹: {model_id}")
        
        # åŠ è½½æ¨¡å‹
        model = load_model_for_prediction(model_id)
        if model is None:
            return jsonify({"error": f"æ— æ³•åŠ è½½æ¨¡å‹: {model_id}"}), 404
        
        # é¢„å¤„ç†å›¾åƒ
        input_tensor = preprocess_canvas_image(image_base64)
        if input_tensor is None:
            return jsonify({"error": "å›¾åƒé¢„å¤„ç†å¤±è´¥"}), 400
        
        # æ‰§è¡Œæ¨ç†
        prediction, probabilities = perform_inference(model, input_tensor)
        
        result = {
            "prediction": int(prediction),
            "probabilities": probabilities.tolist(),
            "confidence": float(probabilities.max())
        }
        
        print(f"âœ… é¢„æµ‹å®Œæˆ: {prediction} (ç½®ä¿¡åº¦: {probabilities.max():.4f})")
        return jsonify(result)
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
        return jsonify({"error": f"é¢„æµ‹å¤±è´¥: {str(e)}"}), 500

def load_model_for_prediction(model_id):
    """åŠ è½½æŒ‡å®šçš„æ¨¡å‹ç”¨äºé¢„æµ‹
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆå®Œæ•´æ–‡ä»¶åï¼Œä¸å«.pthæ‰©å±•åï¼‰
    
    Returns:
        torch.nn.Module: åŠ è½½çš„æ¨¡å‹ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if model_id in LOADED_MODELS:
            print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹: {model_id}")
            return LOADED_MODELS[model_id]
        
        # æ„å»ºæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨backend/saved_models
        if os.path.exists('backend/saved_models'):
            model_path = os.path.join('backend/saved_models', f"{model_id}.pth")
        elif os.path.exists('saved_models'):
            model_path = os.path.join('saved_models', f"{model_id}.pth")
        else:
            model_path = os.path.join('backend/saved_models', f"{model_id}.pth")
        
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
        
        # è§£ææ¨¡å‹ç±»å‹
        model_type = model_id.split('_best_acc_')[0]
        
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_type} ä» {model_path}")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = get_model_instance(model_type)
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(model_path, map_location='cpu')
        model.load_state_dict(checkpoint)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        # ç¼“å­˜æ¨¡å‹ï¼ˆé™åˆ¶ç¼“å­˜æ•°é‡é¿å…å†…å­˜è¿‡å¤šï¼‰
        if len(LOADED_MODELS) >= 5:  # æœ€å¤šç¼“å­˜5ä¸ªæ¨¡å‹
            # åˆ é™¤æœ€æ—§çš„æ¨¡å‹
            oldest_key = next(iter(LOADED_MODELS))
            del LOADED_MODELS[oldest_key]
            print(f"ğŸ—‘ï¸ åˆ é™¤ç¼“å­˜æ¨¡å‹: {oldest_key}")
        
        LOADED_MODELS[model_id] = model
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸå¹¶ç¼“å­˜: {model_id}")
        
        return model
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_id}: {e}")
        return None

def preprocess_canvas_image(image_base64):
    """é¢„å¤„ç†Canvaså›¾åƒæ•°æ®
    
    Args:
        image_base64: base64ç¼–ç çš„å›¾åƒæ•°æ®
    
    Returns:
        torch.Tensor: é¢„å¤„ç†åçš„tensorï¼Œå½¢çŠ¶ä¸º(1, 1, 28, 28)ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # è§£ç base64å›¾åƒ
        if image_base64.startswith('data:image/'):
            # ç§»é™¤data URLå‰ç¼€
            image_base64 = image_base64.split(',')[1]
        
        # base64è§£ç 
        image_data = base64.b64decode(image_base64)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        image = Image.open(io.BytesIO(image_data))
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        if image.mode != 'L':
            image = image.convert('L')
        
        # è°ƒæ•´å¤§å°åˆ°28x28
        image = image.resize((28, 28), Image.LANCZOS)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        image_array = np.array(image)
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        image_array = image_array.astype(np.float32) / 255.0
        
        # åº”ç”¨MNISTæ ‡å‡†å½’ä¸€åŒ–ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        image_array = (image_array - 0.1307) / 0.3081
        
        # è½¬æ¢ä¸ºPyTorch tensor
        image_tensor = torch.from_numpy(image_array)
        
        # æ·»åŠ batchå’Œchannelç»´åº¦: (28, 28) -> (1, 1, 28, 28)
        image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)
        
        print(f"ğŸ“· å›¾åƒé¢„å¤„ç†å®Œæˆï¼Œtensorå½¢çŠ¶: {image_tensor.shape}")
        return image_tensor
        
    except Exception as e:
        print(f"âŒ å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
        return None

def perform_inference(model, input_tensor):
    """æ‰§è¡Œæ¨¡å‹æ¨ç†
    
    Args:
        model: PyTorchæ¨¡å‹
        input_tensor: è¾“å…¥tensor
    
    Returns:
        tuple: (prediction, probabilities) - é¢„æµ‹ç»“æœå’Œæ¦‚ç‡åˆ†å¸ƒ
    """
    try:
        with torch.no_grad():
            # å‰å‘ä¼ æ’­
            outputs = model(input_tensor)
            
            # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
            probabilities = torch.softmax(outputs, dim=1)
            
            # è·å–é¢„æµ‹ç»“æœ
            prediction = torch.argmax(probabilities, dim=1)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            probabilities = probabilities.squeeze().numpy()
            prediction = prediction.item()
            
            return prediction, probabilities
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        raise e

@app.route('/api/history', methods=['GET'])
def get_training_history():
    """è·å–è®­ç»ƒå†å²è®°å½•"""
    try:
        # è¿”å›æœ€è¿‘çš„è®­ç»ƒä»»åŠ¡è®°å½•
        with TRAINING_LOCK:
            history = []
            for job_id, job_info in TRAINING_JOBS.items():
                history.append({
                    "job_id": job_id,
                    "model_id": job_info['model_id'],
                    "status": job_info['status'],
                    "start_time": job_info['start_time'],
                    "end_time": job_info.get('end_time'),
                    "final_accuracy": job_info['progress'].get('best_accuracy', 0.0),
                    "config": job_info['config']
                })
            
            # æŒ‰å¼€å§‹æ—¶é—´é™åºæ’åº
            history.sort(key=lambda x: x['start_time'], reverse=True)
            
            return jsonify(history)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def check_system_health():
    """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        health_info = {
            "status": "healthy",
            "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S"),
            "active_training_jobs": len([job for job in TRAINING_JOBS.values() if job['status'] == 'running']),
            "loaded_models": len(LOADED_MODELS),
            "available_models": len(AVAILABLE_MODELS)
        }
        
        return health_info
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time_module.strftime("%Y-%m-%dT%H:%M:%S")
        }

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨MNISTæ™ºèƒ½åˆ†æå¹³å°åç«¯æœåŠ¡...")
    print("ğŸ”§ ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
    
    health = check_system_health()
    print(f"âœ… ç³»ç»ŸçŠ¶æ€: {health['status']}")
    print(f"ğŸ“Š å¯ç”¨æ¨¡å‹: {health['available_models']} ä¸ª")
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(host='0.0.0.0', port=5000, debug=True) 